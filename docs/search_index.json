[["index.html", "An Introduction to Statistics Welcome", " An Introduction to Statistics Mariefel Nicole Deypalan April 30, 2021 Welcome This site contains notes intended for an introductory course in Statistics. It begins by establishing some definitions about data and probability, and then goes on to introduce hypothesis tests. These materials hope to provide a foundation for exploring other inferential tools in Statistics. "],["introduction-to-data.html", "Unit 1 Introduction to Data 1.1 What is Data? 1.2 Types of Data 1.3 Homework 1.4 Measures of Central Tendency4 1.5 Measures of Variability 1.6 Examining Numerical Data 1.7 Examining Categorical Data 1.8 Aside: Some Number Theory", " Unit 1 Introduction to Data 1.1 What is Data? Data can be thought of simply as information, a collection of facts. An example would be this Star Wars dataset1 that will be used for illustrative purposes. This is a collection of information about the characters in Star Wars such as height, hair color, and species. Each row is one observation. 1.2 Types of Data In a more technical sense, data is a set of values of qualitative or quantitative variables about one or more persons or objects.2 The Star Wars dataset will be used to illustrate this concept. The dataset contains 14 columns. Each column corresponds to a variable. A variable in statistics means an attribute. For example, height and hair color are variables and have a value for each observation. Variables may be quantitative or qualitative. Numerical variables are those that take on quantitative values and represent some kind of measurement. The variable mass is an example of a quantitative variable. Categorical variables, on the other hand, have qualitative values. Each value can be thought of as a category or label. An example would be eye_color from the Star Wars dataset, which has the following values: &quot;blue&quot; &quot;yellow&quot; &quot;red&quot; &quot;brown&quot; &quot;blue-gray&quot; &quot;black&quot; &quot;orange&quot; &quot;hazel&quot; &quot;pink&quot; &quot;unknown&quot; &quot;red, blue&quot; &quot;gold&quot; &quot;green, yellow&quot; &quot;white&quot; &quot;dark&quot; 1.3 Homework The Motor Trend Car Road Tests dataset3 comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles. Classify each variable in this dataset as numerical or categorical. mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 mpg - Miles/(US) gallon cyl - Number of cylinders disp - Displacement (cu.in.) hp - Gross horsepower drat - Rear axle ratio wt - Weight (1000 lbs) qsec - 1/4 mile time vs - Engine (0 = V-shaped, 1 = straight) am - Transmission (0 = automatic, 1 = manual) gear - Number of forward gears carb - Number of carburetors 1.4 Measures of Central Tendency4 The mean, median, and mode are known as measures of central tendency in statistics and are called such because they represent what is a typical or central value. They can be thought of as average values. Mean - the sum of all measurements divided by the number of observations in the dataset Median - the middle value that separates the higher half from the lower half of the dataset Mode - the most frequent value in the data set 1.5 Measures of Variability The range, interquartile range (IQR), variance, and standard deviation are known as the measures of variability. They describe how far away observations tend to fall from the center.5 Range - the difference between the maximum and the minimum IQR - the difference between the third quartile and the first quartile variance - the average squared deviation of each number from the mean of a data set Standard deviation - the square root of the variance 1.6 Examining Numerical Data 1.6.1 Numerical Methods The most common way to explore numerical data numerically is by calculating what are called summary statistics. A summary statistic is a quantity that summarizes the data into a single point in order to present information as simply as possible. The three measures of central tendency are examples of summary statistics, and there are several others which might be familiar. One common set of summary statistics is known as the five-point summary, and is comprised of: minimum first quartile (25th percentile) median third quartile (75th percentile) maximum To illustrate, here is a five-point summary for the mass of the Star Wars characters: Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 15.00 55.60 79.00 97.31 84.50 1358.00 28 The last column labeled NA's gives the number of unknown values. This is not part of the five-point summary, but is a useful addition because it shows how many values were missing. There are 87 observations in the Star Wars dataset, but since 28 values for mass are missing, only 59 values were considered in calculating the five-point summary above. 1.6.2 Graphical Methods Numerical data can be visualized and examined using different types of graphs. Histograms and boxplots are two of common types of graphs used to visualize numerical variables. Below is a histogram of the heights of Star Wars characters: The horizontal (or x-) axis of the histogram corresponds to the variable of interest. The y-axis displays the counts of observations that fall within each interval or bin. Note that in constructing the histogram, the left boundaries or endpoints are not included. For example, there are 20 characters (tallest bar) with a height greater than 175 cm but not greater than 185 cm. Those with a height of exactly 175 cm are included in the previous bar. Histograms show the distribution of data. A more technical definition of the term will be introduced later in the course but for now, it would suffice to think of distributions as showing possible values of the data and how often they occur6. From the histogram, one can see which values of the variable are more and less likely. In the example, most characters are at least 175 cm in height but are not taller than 185 cm. Also, there are characters who are significantly taller than the rest, as evidenced by the rightmost bar on the graph. Boxplots are another way of graphing numerical data, and they present different information from histograms. Below is a boxplot of the heights of Star Wars characters: Instead of presenting the frequency of the data, boxplots show the summary statistics presented in the previous section: median - the line in the middle Q1/first quartile - left edge of the box Q3/third quartile - right edge of the box The black points in the plot correspond to outliers, which are data points that deviate significantly from the other observations. In the context of this example, the points on the right represent the extremely short characters (like Yoda) while the points on the left are the towering characters (like Yarael Poof). 1.7 Examining Categorical Data 1.7.1 Tabular Methods Two types of tables are used most commonly in summarizing categorical data. These are frequency tables and contingency tables. 1.7.1.1 Frequency Tables A frequency table shows numbers and percentages for each value7 of the categorical variable. In effect, it presents the distribution of observations across all possible responses. As an example, here is a frequency table for the hair_color variable in the Star Wars dataset. starwars$hair_color : Frequency %(NA+) %(NA-) none 37 42.5 45.1 brown 18 20.7 22.0 black 13 14.9 15.9 &lt;NA&gt; 5 5.7 0.0 blonde 4 4.6 4.9 white 4 4.6 4.9 auburn 1 1.1 1.2 auburn, grey 1 1.1 1.2 auburn, white 1 1.1 1.2 brown, grey 1 1.1 1.2 grey 1 1.1 1.2 unknown 1 1.1 1.2 Total 87 100.0 100.0 The percentages displayed are called relative frequencies and are calculated based on the total number of observations. Consider the row for &lt;NA&gt;, which refers to missing values in the data set. The %(NA+) column tells us that 5.7% of the total observations have missing values for hair color. Looking at the %(NA-) column, the percentage is now 0. That is because this column gives percentages relative to the total observations minus the missing values. 1.7.1.2 Contingency Tables A contingency table displays the relationship between one categorical variable and another. It is called such because it allows us to examine whether the values of one variable are contingent (dependent) upon those of another.8 Here is a contingency table for the variables gender and hair color: feminine masculine Sum auburn 1 0 1 auburn, grey 0 1 1 auburn, white 0 1 1 black 3 9 12 blonde 1 3 4 brown 6 11 17 brown, grey 0 1 1 grey 0 1 1 none 5 31 36 unknown 0 0 0 white 1 3 4 Sum 17 61 78 1.7.2 Graphical Methods Categorical data can also be presented using graphs. The most common types are bar charts and pie charts. 1.7.2.1 Bar Charts Shown here is a bar chart for the hair color of Star Wars characters: The bar chart is the equivalent of a histogram for categorical variables. Each bar will correspond to a row in the frequency table from the previous section. Several facts can be deduced from this graph. Most characters in the dataset have no hair but among those that do, the most common hair color is brown. 1.7.2.2 Exercise: What other facts about the data does this bar chart show? Give at least one. 1.7.2.3 Pie Chart Below is a pie chart for the same variable, hair color, which was presented in the previous sections. Although pie charts are commonly seen in infographics and posters, it is not considered as a good visualization because there is no scale present. The angles of the wedges cannot be compared directly, which makes it difficult to deduce whether one piece of the pie is larger than the other. Comparing this to the bar graph in the previous section, it is easy to see which chart is more effective at conveying information. There are several resources online which explain the ineffectiveness of pie charts in much greater detail. An article in Business Insider explains very well why using a pie chart is a bad visualization technique. 1.8 Aside: Some Number Theory Digressing from the discussion on data, this section will introduce some basic notations in number theory in preparation for the discussion on probability distributions. 1.8.1 The Factorial In mathematics, the factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\): \\[n! = n \\times (n - 1) \\times (n - 2) \\times (n - 3) \\times \\dotsc \\times 3 \\times 2 \\times 1\\] For example, \\(3!\\) is equal to \\(3 \\times 2 \\times 1 = 6\\), while \\(5!\\) is \\(5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\). By definition, \\(0!\\) is 1. 1.8.2 Combinations A combination is a mathematical technique that determines the number of possible arrangements in a collection of items where the order of the selection does not matter.9 Consider a small office with 7 employees. Since the company provides free lunch, a lunch committee consisting of 3 members is needed to decide the weekly menu. How many possible commitees of 3 can be created? The above is an example of a combination problem. It is equivalent to counting the number of ways a group of 3 can be chosen from a bigger group of 7. In mathematics, this is denoted as \\({7 \\choose 3}\\) and read as 7 choose 3 or more generally, \\({n \\choose k}\\) where \\(n\\) is the total number of items and \\(k\\) the number of items to be selected. To obtain the value of such an expression, the following formula is used: \\[{n \\choose k} = \\frac{n!}{r! (n - r)!}\\] Using this formula, the total number of possible committees is: \\[ \\begin{align} {n \\choose k} &amp;= \\frac{n!}{r! (n - r)!} \\\\ {7 \\choose 3} &amp;= \\frac{7!}{3! (7 - 3)!} \\\\ {7 \\choose 3} &amp;= \\frac{7!}{3! \\cdot 4!} \\\\ {7 \\choose 3} &amp;= \\frac{7 \\cdot \\cancel{6} \\cdot 5 \\cdot \\cancel{4!}}{\\cancel{3!} \\cdot \\cancel{4!}} = 35 \\end{align} \\] Answer: There are 35 possible ways of selecting a lunch committee of 3 from a group of 7. Tidyverse. Wikipedia. mtcars. Central tendency. Measures of Variability. 365 Data Science. Statistics: Presenting Categorical Data Statistics: Presenting Categorical Data Combinations. "],["probability-and-probability-distributions.html", "Unit 2 Probability and Probability Distributions 2.1 Defining Probability 2.2 Homework 1 2.3 Independence 2.4 Random Variables 2.5 Homework 2 2.6 PMFs and PDFs 2.7 The Uniform Distribution 2.8 The Binomial Distribution 2.9 Homework 3", " Unit 2 Probability and Probability Distributions 2.1 Defining Probability Probability The measure of how likely something is to occur To begin talking about this topic, three definitions will be introduced: experiment - any process, real or hypothetical, in which the possible outcomes can be identified ahead of time10 event - a well-defined set of possible outcomes of the experiment11 sample space - a collection or a set of possible outcomes or results of a random experiment The simplest way of calculating the probability that an event \\(E\\) occurs is: \\[P(E) = \\frac{\\text{number of desired outcomes}}{\\text{total possible outcomes}}\\] 2.2 Homework 1 The concept of probability can be better illustrated through examples. Below are some situations where probability is of interest. For each example, identify the experiment, the event and calculate the probability. What is the probability that a coin toss comes up heads? When a die is rolled, what is the probability of getting an even number? When two dice are rolled, what is the probability of rolling a double? In a standard deck of 52 cards, what is the probability of drawing a face card? In a drawer containing 3 pairs of black socks and 4 pairs of white socks, what is the probability of drawing a black pair? 2.3 Independence In talking about probability, independence is a term that often comes up. In statistics, two events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of occurrence of the other12. Here are some examples of independent events: Flipping a fair coin and rolling a die Drawing a card from a deck, putting it back, and drawing another card Another related concept is that of disjoint events. Disjoint, or mutually exclusive events, are events that cannot occur at the same time. If two events \\(A\\) and \\(B\\) are disjoint, the occurrence of \\(A\\) means that \\(B\\) cannot occur and vice versa. Examples: Getting both heads and tails in one coin flip Having one pet that is both a dog and a bird (considering only real species) 2.4 Random Variables Random variables are very important building blocks of probability theory, but this course only requires an informal definition. Here and in the next chapters, mathematical underpinnings have been excluded where permissible to keep the content easily understandable. What is a random variable? It is a variable whose values depend on the outcome of some experiment.13 This informal definition combines two important terms previously introduced - variable and experiment. In statistical equations, random variables are typically represented by capital letters. Random variables are best described by probability distributions. Again, building off of previously defined terms, probability distributions are, simply put, distributions of probability. More precisely, a probability distribution is a mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment14. 2.4.1 Discrete and Continuous Random variables can either be discrete or continuous. This classification pertains to the types of values a random variable can take. Discrete random variables are those that have distinct, countable values. Continuous random variables, on the other hand, take on any value within an interval, i.e. infinitely many values. As with everything else, this can be better understood with examples. Below is an exercise that will require the application of the conceptual definitions stated above. 2.5 Homework 2 Classify each random variable as discrete or continuous. \\(X\\), a random variable that has a value of 1 if a coin toss comes up heads, 0 otherwise A random variable \\(Y\\) which takes on the number of times an even number comes up in 10 rolls of a die A random variable \\(C\\) which is the exact temperature in a certain room at 10:20:00 AM \\(M\\), the exact mass of a randomly selected person in the class \\(K\\), the mass of a randomly selected person in a mall rounded to the nearest gram 2.6 PMFs and PDFs The probability mass function or pmf is a function that gives the probability that a discrete random variable is exactly equal to some value. The continuous analog of pmfs are pdfs or probability density functions. 2.7 The Uniform Distribution The uniform distribution has two forms - discrete and continuous. As the name implies, the uniform distribution describes an experiment where all possible outcomes in the sample space are equally likely to occur. To differentiate between the discrete uniform and continuous uniform distributions, it is important to look at what kind of experiment is generating the data and what kind of values are possible given that process. Suppose we are drawing a random card from a deck of 52. This is a discrete process because we can count the number of possible outcomes. Since each card has an equal chance of being drawn, a random variable \\(R\\) corresponding to the card that will be drawn is uniformly distributed, with pmf given by: \\[p(x) = \\frac{1}{N}, \\hspace{10mm} N = 1, 2, 3, \\dotsc\\] \\(N\\) is the total number of possible outcomes. The continuous uniform distribution, on the other hand, can be illustrated by considering an interval on the number line, say \\((1,5)\\). consider a random variable \\(B\\) that is uniformly distributed in this interval. This means that \\(B\\) is equally likely to take on any value between 1 and 5. (Here, any value includes fractions and irrational numbers.) The pdf of the continuous uniform distribution is given by \\[f(x) = \\frac{1}{b - a}, \\hspace{10mm} a &lt; x &lt; b\\] where \\(a\\) and \\(b\\) are the bounds of the interval. Note that the equation above is not dependent on \\(x\\), i.e. regardless of the value of the random variable, the probability will be the same. When a random variable is uniformly distributed across an interval, each and every value in the interval is equally likely to occur. In the example, every number in the interval \\((1, 5)\\) has a \\(\\frac{1}{4}\\) chance of occurring. Suppose a number is selected at random from this interval. What is the probability that the number chosen will be between 2 and 4? Let \\(i\\) and \\(j\\) be the bounds of the desired interval. \\[ \\begin{align} P(E) &amp;= (j - i) \\cdot \\frac{1}{b-a} \\\\ &amp;= (4 - 2) \\cdot \\frac{1}{5 - 1} \\\\ &amp;= 2 \\cdot \\frac{1}{4} = \\frac{1}{2} \\end{align} \\] 2.8 The Binomial Distribution 2.8.1 Bernoulli Trial A Bernoulli trial is an experiment that has only two possible outcomes - success and failure. A success is denoted by 1 and a failure by 0. Let \\(\\pi\\) be the probability of success, \\(0 &lt; \\pi &lt; 1\\). This is perhaps the simplest experiment15, but gives rise to an entire family of discrete distributions. Of these, the most common is the BINOMIAL DISTRIBUTION whose probability mass function is introduced below. 2.8.2 PMF of a Binomial Distribution16 Let \\(X\\) be the number of 1s or successes in \\(n\\) independently performed Bernoulli trials. \\(X\\) then has a binomial distribution, denoted by \\(X \\sim \\text{Bi}(n, \\pi)\\), and with a pmf given by: \\[p(k) = {n \\choose k} \\pi^k(1 - \\pi)^{n-k}, \\hspace{10mm} k =0, 1, 2, \\dotsc, n.\\] The parameters are \\(n\\), the total number of trials, and \\(\\pi\\) which represents the probability of success. \\(k\\) can be thought of as the value of the random variable. As an illustrative example, consider a coin toss. There are only two possible outcomes - heads or tails. This can be considered a Bernoulli trial where success is defined as getting heads and failure is getting tails. To calculate the probability of success, the basic definition of probability introduced at the beginning of this unit can be used. \\[ \\begin{align} P(E) &amp;= \\frac{\\text{number of desired outcomes}}{\\text{total possible outcomes}} \\\\ P(\\text{heads}) &amp;= \\frac{1}{2} = 0.5 = \\pi \\end{align} \\] Suppose that the number of heads in 5 coin tosses is of interest. This random variable \\(X\\) fits the definition of a binomially distributed random variable, \\(X \\sim \\text{Bi}(5, 0.5)\\). What is the probability of getting 2 heads in 5 coin tosses? To answer the question above, the given simply need to be substituted into the pmf of the binomial distribution given earlier: \\[ \\begin{align} p(k) &amp;= {n \\choose k} \\pi^k (1 - \\pi)^{n-k} \\\\ p(2) &amp;= {5 \\choose 2} (0.5)^2 (1 - 0.5)^{5 - 2} \\\\ p(2) &amp;= {5 \\choose 2} (0.5)^2 (0.5)^3 = 0.3125 \\end{align} \\] 2.9 Homework 3 Read about skewness in distributions and define the following terms: left-skewed right-skewed Highlight the differences between the two. DeGroot, M. H., &amp; Schervish, M. J. (2012). Probability and Statistics. Pearson Education. DeGroot, M. H., &amp; Schervish, M. J. (2012). Probability and Statistics. Pearson Education. Independence. Random variable. Probability distribution. Bataller, R (2013). Elementary Probability Theory for Math/AMF Majors. Mathematics Department, School of Science and Engineering, Ateneo de Manila University. Bataller, R (2013). Elementary Probability Theory for Math/AMF Majors. Mathematics Department, School of Science and Engineering, Ateneo de Manila University. "],["the-normal-distribution-and-the-central-limit-theorem.html", "Unit 3 The Normal Distribution and the Central Limit Theorem 3.1 Population vs. Sample 3.2 The Normal Distribution 3.3 The Empirical Rule 3.4 The Central Limit Theorem 3.5 Concluding Remark", " Unit 3 The Normal Distribution and the Central Limit Theorem 3.1 Population vs. Sample Population - all the elements from a set of data Sample - one or more observations taken from the population A quantity taken for the entire population is known as a population parameter, while that taken for a sample is called a sample statistic. Sampling Distribution The sampling distribution of a statistic is a probability distribution based on a large number of samples of size \\(n\\) from a given population.17 This means that The sampling distribution is a probability distribution. From the population, many samples of the same size are taken and a statistic (e.g mean, proportion) is taken for each sample. Using the values of the statistic that were calculated from the samples, a distribution is then obtained. Standard Error The measure of the variability of the sample means \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\), \\(\\sigma\\) is the population standard deviation 3.2 The Normal Distribution The normal distribution is the most common among all probability distributions, perhaps because it describes a lot of variables quite well. According to an introductory statistics book, Many variables are nearly normal, but none are exactly normal. Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems.18 The normal distribution is bell-shaped. It is symmetric and unimodal; it has one peak and tapers off at both ends in exactly the same way. Below is an example of a normal distribution with a mean of 2 and a standard deviation of 1. Two parameters are used to describe the normal distribution - mean \\(\\mu\\) and standard deviation \\(\\sigma\\). A mean of 0 and a standard deviation of 1 corresponds to the standard normal distribution which is shown below: 3.2.1 PDF of a Normal Distribution The normal distribution is continuous, i.e. a normally distributed random variable can take on any value between \\((-\\infty, +\\infty)\\). The pdf for the normal distribution is: \\[f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp{\\left[-\\frac{1}{2} \\cdot \\left( \\frac{x - \\mu}{\\sigma}\\right)^2\\right]}\\] 3.2.2 The Z-score measures how many standard deviations above or below the mean a data point is Formula for the z-score: \\[z = \\frac{x-\\mu}{\\sigma}\\] The above equation looks like the last part of the normal pdf. Substituting this expression for z yields: \\[f(z) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp{\\left[-\\frac{1}{2} \\cdot z^2\\right]}\\] This is the equation for the standard normal distribution. These equations are rarely used to compute probabilities because various software have made these available through the use of functions. For example, the Excel function NORMDIST()19 gives the exact value of the pdf for any normal distribution specified by a mean and standard deviation. However, introducing these concepts is still important to justify the robustness of the statistical tools that will be discussed later on. Knowing these fundamental concepts allows one to confidently use and interpret the results of many different methods such as hypothesis testing, linear regression, etc. 3.3 The Empirical Rule Another useful property of normally distributed data is given by the empirical rule. Given that the distribution of the data is bell-shaped, this rule states that: Approximately 68% of the data lie within 1 standard deviation from the mean Approximately 95% of the data, 2 standard deviations About 99.7% of the data, 3 standard deviations Figure 3.1: The Empirical Rule 3.4 The Central Limit Theorem 3.4.1 Conditions for the CLT The Central Limit Theorem (CLT) states that for a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), taking sufficiently large random samples with replacement and computing sample means will yield a distribution of sample means (sampling distribution) that is approximately normal.20 The CLT holds true provided that the following conditions are met: Independence: The sampled observations must be independent. Sample size/skew: If the population is skewed, the sample size \\(n\\) must be greater than 3. If not, the population distribution must be normal. Under this theorem, the parameters for the sampling distribution are \\(mu\\), the population mean, and \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\), the standard error. That is, \\(\\overline{X} \\sim N(\\text{mean} = \\mu, \\sigma = \\text{SE})\\). 3.4.2 Applying the CLT21 Suppose my iPod has 3,000 songs. I know that the distribution of lengths of these songs is right-skewed, and for this iPod, the mean length is 3.45 minutes and the standard deviation is 1.63 minutes. Im about to take a trip to visit my parents and the drive is 6 hours. I make a random playlist of 100 songs. What is the probability that my playlist lasts the entire drive? Given: \\(\\mu\\) = 3.45 minutes \\(\\sigma\\) = 1.63 minutes 6 hours = 360 minutes Find: probability that 100 randomly selected songs lasts 360 minutes, which is the same as probability that the average length of the 100 randomly selected songs is at least \\(360/100 = 3.6\\) minutes \\(P(\\overline{X} \\geq 3.6)\\) According to the CLT: \\[ \\overline{X} \\sim N(\\mu = 3.45, ~ \\text{SE} = \\frac{1.63}{\\sqrt{100}} = 0.163) \\] \\[Z = \\frac{x - \\mu}{\\sigma} = \\frac{3.6 - 3.45}{0.163} = 0.92\\] \\[P(Z \\geq 0.92) = 0.1788\\] 3.5 Concluding Remark This unit has introduced one of the most important theorems in statistics and in doing so, has inevitably scratched the surface of one of the most fundamental inferential methods - hypothesis testing. The next unit will more formally introduce the rudiments of hypothesis testing - from setting up hypotheses to selecting the appropriate confidence level. After establishing the fundamental definitions, applications and examples will be presented which will hopefully solidify the previous discussions. The normal distribution will be re-introduced and in the context of hypothesis testing, its ubiquity in the realm of inferential statistics will be more apparent. Sampling Distributions. Diez, D., Centinkaya-Rundel, M., &amp; Barr, C. (2019). OpenIntro Statistics. OpenIntro. NORMDIST Function. Central Limit Theorem. Inferential Statistics, Coursera. "],["confidence-intervals-and-hypothesis-testing.html", "Unit 4 Confidence Intervals and Hypothesis Testing 4.1 Confidence Intervals 4.2 Hypothesis Testing 4.3 Simple and Composite Hypotheses 4.4 Testing Hypotheses", " Unit 4 Confidence Intervals and Hypothesis Testing 4.1 Confidence Intervals 4.1.1 What are confidence intervals? Confidence intervals give us a range of plausible values for the population parameter based on results from a sample. The conditions for the CLT must also be met for the confidence interval to be valid. 4.1.2 Constructing a Confidence Interval The confidence interval for the mean is computed as: \\[(\\overline{x} - z^* \\frac{s}{\\sqrt{n}}, ~ \\overline{x} + z^* \\frac{s}{\\sqrt{n}})\\] \\(\\overline{x}\\) is the sample mean \\(s\\) is the sample mean \\(n\\) is the sample size \\(z^*\\) is the critical z-score 4.2 Hypothesis Testing Suppose one is interested in finding out whether one advertising method, say X, is more effective than a similar method Y in increasing the sales of a business. Is X method more effective compared to method Y? A simple way to answer this is to collect data for both Method X and Method Y. Suppose we randomly select a day or week, and count the number of sales attributable to each method during this period. We could then get the average of both methods and take the difference. We can say that Method X is better if the difference is positive, or worse if the difference is negative. This seems simple, but remember that the values will vary each time the data is collected. Hence, the difference in the averages will also be different and we cannot be 100% certain of the magnitude and direction of the difference. With the uncertainties that are inherent in doing an experiment, how then can answer such questions as the one above in a principled way? The answer is Statistics. Hypothesis testing is a tool used to obtain statistical evidence to arrive at certain decisions given the data, accounting for uncertainty. 4.2.1 The Null and Alternative Hypotheses Suppose we have a space \\(\\Omega\\) where all possible parameters \\(\\theta\\) about the data can be found. We can then divide this set into two mutually exclusive sets which we call \\(\\Omega_0\\) and \\(\\Omega_1\\). We shall then denote \\(H_0\\) as the hypothesis that \\(\\theta \\in \\Omega_0\\), while \\(H_1\\) is the hypothesis when \\(\\theta \\in \\Omega_1\\). In statistical literature, these two hypotheses are called the null hypothesis and the alternative hypothesis, respectively. 4.2.2 Decision Errors Since there are two hypotheses and these are disjoint, only one hypothesis can be true. If the wrong hypothesis is taken to be true, a loss or cost is incurred. Suppose that the \\(\\theta \\in \\Omega_0\\) or the null hypothesis is true, but it is rejected. This is called a Type I error. If instead \\(\\theta \\in \\Omega_1\\) or the alternative hypothesis is true, but we do not reject the null hypothesis, we are making a Type II error. For most cases, we can set our hypothesis test to have a certain Type I error rate, \\(\\alpha\\) (a number from 0 to 1), which corresponds to the probability of committing a Type I error. Thus, if we have a test with \\(\\alpha = 0.05\\), it means that we have at most a 5% chance of rejecting the null hypothesis when the null hypothesis is true. 4.2.3 Setting Up \\(H_0\\) and \\(H_1\\) Using the advertising example above, suppose that the hypothesis is \\(\\mu_X &gt; \\mu_Y\\) with respect to sales, i.e. Method X generates more sales that Method Y. Analysts could claim that the data supports the theory that \\(\\mu_X &gt; \\mu_Y\\), when in fact \\(\\mu_X \\leq \\mu_Y\\) (Case 1). They could also mistakenly claim that data fails to support \\(\\mu_X\\) &gt; $_Y when it is true (Case 2). Both cases entail a rejection of the null hypothesis and for both situations, the setup of the null and alternative hypothesis are different, as shown below. For Case 1: \\[ \\begin{aligned} H_0 &amp;: \\mu_{X} \\leq \\mu_{Y}\\\\ H_1 &amp;: \\mu_{X} &gt; \\mu_{Y} \\end{aligned} \\] For Case 2: \\[ \\begin{aligned} H_0 &amp;: \\mu_{X} \\geq \\mu_{Y}\\\\ H_1 &amp;: \\mu_{X} &lt; \\mu_{Y} \\end{aligned} \\] It is helpful to think about the consequence of mistakenly rejecting the null hypothesis, i.e., committing a type I error. In most business cases, it is costly to introduce a new method as it will likely entail higher costs during implementation than retaining the old method. Thus, mistakenly rejecting the null hypothesis of Case 1 will be more costly to the business than 2 as Case 2 is about retaining the current strategy. Intuitively, \\(H_0\\) represents the status quo or current situation (no difference, hence the equality) and \\(H_1\\) asserts that there is a difference. This is why in practice, \\(H_0\\) must always contain some form of equality (\\(=\\), \\(\\leq\\), \\(\\geq\\)) and \\(H_1\\) must be stated in a way that complements \\(H_0\\) exactly. 4.3 Simple and Composite Hypotheses A simple hypothesis is where the parameter \\(\\theta\\) has only one value in either \\(\\Omega_0\\) or \\(\\Omega_1\\). Such a setup is shown below: \\[ \\begin{aligned} H_0 &amp;: \\theta=\\theta_0 \\\\ H_1&amp;: \\theta \\neq \\theta_0 \\end{aligned} \\] \\(\\theta_0\\) is the parameter value of the null hypothesis set. Statistical tests that seek to test this hypothesis setup are called two-sided hypothesis tests. A composite hypothesis, on the other hand, is a setup where the hypothesis space of either \\(\\Omega_0\\) or \\(\\Omega_1\\) contains more than one value for \\(\\theta_0\\). There are two ways to set up a composite hypothesis and the difference lies in the inequality sign used. One could either do: \\[ \\begin{aligned} H_0 &amp;: \\theta\\leq\\theta_0 \\\\ H_1&amp;: \\theta &gt; \\theta_0 \\end{aligned} \\] or the opposite which is: \\[ \\begin{aligned} H_0 &amp;: \\theta\\geq\\theta_0 \\\\ H_1&amp;: \\theta &lt; \\theta_0 \\end{aligned} \\] Statistical tests for composite hypotheses are called one-sided tests and, depending on the inequality sign of the alternative hypothesis, it can either be called a right-tailed (first) or a left-tailed test (second). 4.4 Testing Hypotheses 4.4.1 Using a Critical Value \\(c\\) In doing a hypothesis test, we either decide to reject or not reject the null hypothesis. To do so, we need to define a statistic \\(T\\) as the distance between the sample statistic and \\(\\theta_0\\). \\(T\\) is thus random (as our data is a random sample of our population). Given a particular value of \\(\\theta_0\\), we might want to have a threshold \\(c\\), which we will call the critical value. Using \\(c\\) we can decide to reject \\(H_0\\) if \\(T \\geq c\\), or not reject \\(H_0\\) if \\(T &lt; c\\). Each threshold corresponds to a value of \\(\\alpha\\). The correct way of testing hypotheses is to set the \\(\\alpha\\) first and then use the corresponding critical value \\(c\\). The mathematics behind this is ommitted for now as it can get quite overwhelming. If this has piqued curiosity, however, a number of introductory statistics texts and resources on the Internet explore the mathematical underpinnings of hypothesis testing in detail. The applications of this concept will be shown in the unit on z-tests and t-tests. 4.4.2 Visualizing Rejection Regions Values that are greater than or equal to a certain threshold \\(c\\) that intersect with the alternative hypothesis parameter space \\(\\Omega_1\\) are contained within the rejection region. Rejection regions in the visualizations below are colored in blue. For a two-sided hypothesis test, we reject on both tails for a symmetric distribution (usually a normal distribution). This is so since an extremely high or low sample statistic can be evidence for us to reject \\(H_0\\) (as implied by the \\(\\neq\\) sign). Since the probability of rejecting a true \\(H_0\\) supposedly covers both high and low values, the critical value \\(c\\) to be used should correspond to \\(\\alpha/2\\), not \\(\\alpha\\). For one-sided tests, we reject depending on the parameter space of the alternate hypothesis. Hence, the rejection region for a left-tailed test can be shown as: The rejection region for a right-tailed test is shown below: 4.4.3 The p-value Another important concept is the p-value which is often reported in analyses to denote statistical significance relative to a set \\(\\alpha\\) (often 0.05). It is defined as the probability of obtaining test results at least as extreme as the results actually observed when \\(H_0\\) is true. It can be thought of as a measure of how surprised you are of the data. Higher values mean that the data is not at all surprising relative to the null hypothesis. Although this might be the case, p-values do not dictate the probability that the null hypothesis is true given the data, and cannot be used to draw conclusions on how likely the null hypothesis is compared to the alternative, and vice versa. It only allows one to decide whether or not to reject \\(H_0\\). Nonetheless, we can by definition, use p-values to test for hypothesis. This is done by calculating the p-value and comparing it with a set \\(\\alpha\\) level. We reject the null hypothesis if the p-value \\(p \\leq \\alpha\\). "]]
