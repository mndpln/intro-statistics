[["index.html", "An Introduction to Statistics Welcome", " An Introduction to Statistics Mariefel Nicole Deypalan May 17, 2021 Welcome This site contains notes intended for an introductory course in Statistics. It begins by establishing some definitions about data and probability, and then goes on to introduce hypothesis tests. These materials hope to provide a foundation for exploring other inferential tools in Statistics. "],["introduction-to-data.html", "Unit 1 Introduction to Data 1.1 What is Data? 1.2 Types of Data 1.3 Homework 1.4 Measures of Central Tendency4 1.5 Measures of Variability 1.6 Examining Numerical Data 1.7 Examining Categorical Data 1.8 Aside: Some Number Theory", " Unit 1 Introduction to Data 1.1 What is Data? Data can be thought of simply as information, a collection of facts. An example would be this Star Wars dataset1 that will be used for illustrative purposes. This is a collection of information about the characters in Star Wars such as height, hair color, and species. Each row is one observation. 1.2 Types of Data In a more technical sense, data is a set of values of qualitative or quantitative variables about one or more persons or objects.2 The Star Wars dataset will be used to illustrate this concept. The dataset contains 14 columns. Each column corresponds to a variable. A variable in statistics means an attribute. For example, height and hair color are variables and have a value for each observation. Variables may be quantitative or qualitative. Numerical variables are those that take on quantitative values and represent some kind of measurement. The variable mass is an example of a quantitative variable. Categorical variables, on the other hand, have qualitative values. Each value can be thought of as a category or label. An example would be eye_color from the Star Wars dataset, which has the following values: &quot;blue&quot; &quot;yellow&quot; &quot;red&quot; &quot;brown&quot; &quot;blue-gray&quot; &quot;black&quot; &quot;orange&quot; &quot;hazel&quot; &quot;pink&quot; &quot;unknown&quot; &quot;red, blue&quot; &quot;gold&quot; &quot;green, yellow&quot; &quot;white&quot; &quot;dark&quot; 1.3 Homework The Motor Trend Car Road Tests dataset3 comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles. Classify each variable in this dataset as numerical or categorical. mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 mpg - Miles/(US) gallon cyl - Number of cylinders disp - Displacement (cu.in.) hp - Gross horsepower drat - Rear axle ratio wt - Weight (1000 lbs) qsec - 1/4 mile time vs - Engine (0 = V-shaped, 1 = straight) am - Transmission (0 = automatic, 1 = manual) gear - Number of forward gears carb - Number of carburetors 1.4 Measures of Central Tendency4 The mean, median, and mode are known as measures of central tendency in statistics and are called such because they represent what is a typical or central value. They can be thought of as average values. Mean - the sum of all measurements divided by the number of observations in the dataset Median - the middle value that separates the higher half from the lower half of the dataset Mode - the most frequent value in the data set 1.5 Measures of Variability The range, interquartile range (IQR), variance, and standard deviation are known as the measures of variability. They describe how far away observations tend to fall from the center.5 Range - the difference between the maximum and the minimum IQR - the difference between the third quartile and the first quartile variance - the average squared deviation of each number from the mean of a data set Standard deviation - the square root of the variance 1.6 Examining Numerical Data 1.6.1 Numerical Methods The most common way to explore numerical data numerically is by calculating what are called summary statistics. A summary statistic is a quantity that summarizes the data into a single point in order to present information as simply as possible. The three measures of central tendency are examples of summary statistics, and there are several others which might be familiar. One common set of summary statistics is known as the five-point summary, and is comprised of: minimum first quartile (25th percentile) median third quartile (75th percentile) maximum To illustrate, here is a five-point summary for the mass of the Star Wars characters: Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 15.00 55.60 79.00 97.31 84.50 1358.00 28 The last column labeled NA's gives the number of unknown values. This is not part of the five-point summary, but is a useful addition because it shows how many values were missing. There are 87 observations in the Star Wars dataset, but since 28 values for mass are missing, only 59 values were considered in calculating the five-point summary above. 1.6.2 Graphical Methods Numerical data can be visualized and examined using different types of graphs. Histograms and boxplots are two of common types of graphs used to visualize numerical variables. Below is a histogram of the heights of Star Wars characters: The horizontal (or x-) axis of the histogram corresponds to the variable of interest. The y-axis displays the counts of observations that fall within each interval or bin. Note that in constructing the histogram, the left boundaries or endpoints are not included. For example, there are 20 characters (tallest bar) with a height greater than 175 cm but not greater than 185 cm. Those with a height of exactly 175 cm are included in the previous bar. Histograms show the distribution of data. A more technical definition of the term will be introduced later in the course but for now, it would suffice to think of distributions as showing possible values of the data and how often they occur6. From the histogram, one can see which values of the variable are more and less likely. In the example, most characters are at least 175 cm in height but are not taller than 185 cm. Also, there are characters who are significantly taller than the rest, as evidenced by the rightmost bar on the graph. Boxplots are another way of graphing numerical data, and they present different information from histograms. Below is a boxplot of the heights of Star Wars characters: Instead of presenting the frequency of the data, boxplots show the summary statistics presented in the previous section: median - the line in the middle Q1/first quartile - left edge of the box Q3/third quartile - right edge of the box The black points in the plot correspond to outliers, which are data points that deviate significantly from the other observations. In the context of this example, the points on the right represent the extremely short characters (like Yoda) while the points on the left are the towering characters (like Yarael Poof). 1.7 Examining Categorical Data 1.7.1 Tabular Methods Two types of tables are used most commonly in summarizing categorical data. These are frequency tables and contingency tables. 1.7.1.1 Frequency Tables A frequency table shows numbers and percentages for each value7 of the categorical variable. In effect, it presents the distribution of observations across all possible responses. As an example, here is a frequency table for the hair_color variable in the Star Wars dataset. starwars$hair_color : Frequency %(NA+) %(NA-) none 37 42.5 45.1 brown 18 20.7 22.0 black 13 14.9 15.9 &lt;NA&gt; 5 5.7 0.0 blonde 4 4.6 4.9 white 4 4.6 4.9 auburn 1 1.1 1.2 auburn, grey 1 1.1 1.2 auburn, white 1 1.1 1.2 brown, grey 1 1.1 1.2 grey 1 1.1 1.2 unknown 1 1.1 1.2 Total 87 100.0 100.0 The percentages displayed are called relative frequencies and are calculated based on the total number of observations. Consider the row for &lt;NA&gt;, which refers to missing values in the data set. The %(NA+) column tells us that 5.7% of the total observations have missing values for hair color. Looking at the %(NA-) column, the percentage is now 0. That is because this column gives percentages relative to the total observations minus the missing values. 1.7.1.2 Contingency Tables A contingency table displays the relationship between one categorical variable and another. It is called such because it allows us to examine whether the values of one variable are contingent (dependent) upon those of another.8 Here is a contingency table for the variables gender and hair color: feminine masculine Sum auburn 1 0 1 auburn, grey 0 1 1 auburn, white 0 1 1 black 3 9 12 blonde 1 3 4 brown 6 11 17 brown, grey 0 1 1 grey 0 1 1 none 5 31 36 unknown 0 0 0 white 1 3 4 Sum 17 61 78 1.7.2 Graphical Methods Categorical data can also be presented using graphs. The most common types are bar charts and pie charts. 1.7.2.1 Bar Charts Shown here is a bar chart for the hair color of Star Wars characters: The bar chart is the equivalent of a histogram for categorical variables. Each bar will correspond to a row in the frequency table from the previous section. Several facts can be deduced from this graph. Most characters in the dataset have no hair but among those that do, the most common hair color is brown. 1.7.2.2 Exercise: What other facts about the data does this bar chart show? Give at least one. 1.7.2.3 Pie Chart Below is a pie chart for the same variable, hair color, which was presented in the previous sections. Although pie charts are commonly seen in infographics and posters, it is not considered as a good visualization because there is no scale present. The angles of the wedges cannot be compared directly, which makes it difficult to deduce whether one piece of the pie is larger than the other. Comparing this to the bar graph in the previous section, it is easy to see which chart is more effective at conveying information. There are several resources online which explain the ineffectiveness of pie charts in much greater detail. An article in Business Insider explains very well why using a pie chart is a bad visualization technique. 1.8 Aside: Some Number Theory Digressing from the discussion on data, this section will introduce some basic notations in number theory in preparation for the discussion on probability distributions. 1.8.1 The Factorial In mathematics, the factorial of a non-negative integer \\(n\\), denoted by \\(n!\\), is the product of all positive integers less than or equal to \\(n\\): \\[n! = n \\times (n - 1) \\times (n - 2) \\times (n - 3) \\times \\dotsc \\times 3 \\times 2 \\times 1\\] For example, \\(3!\\) is equal to \\(3 \\times 2 \\times 1 = 6\\), while \\(5!\\) is \\(5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\). By definition, \\(0!\\) is 1. 1.8.2 Combinations A combination is a mathematical technique that determines the number of possible arrangements in a collection of items where the order of the selection does not matter.9 Consider a small office with 7 employees. Since the company provides free lunch, a lunch committee consisting of 3 members is needed to decide the weekly menu. How many possible commitees of 3 can be created? The above is an example of a combination problem. It is equivalent to counting the number of ways a group of 3 can be chosen from a bigger group of 7. In mathematics, this is denoted as \\({7 \\choose 3}\\) and read as 7 choose 3 or more generally, \\({n \\choose k}\\) where \\(n\\) is the total number of items and \\(k\\) the number of items to be selected. To obtain the value of such an expression, the following formula is used: \\[{n \\choose k} = \\frac{n!}{r! (n - r)!}\\] Using this formula, the total number of possible committees is: \\[ \\begin{align} {n \\choose k} &amp;= \\frac{n!}{r! (n - r)!} \\\\ {7 \\choose 3} &amp;= \\frac{7!}{3! (7 - 3)!} \\\\ {7 \\choose 3} &amp;= \\frac{7!}{3! \\cdot 4!} \\\\ {7 \\choose 3} &amp;= \\frac{7 \\cdot \\cancel{6} \\cdot 5 \\cdot \\cancel{4!}}{\\cancel{3!} \\cdot \\cancel{4!}} = 35 \\end{align} \\] Answer: There are 35 possible ways of selecting a lunch committee of 3 from a group of 7. Tidyverse. Wikipedia. mtcars. Central tendency. Measures of Variability. 365 Data Science. Statistics: Presenting Categorical Data Statistics: Presenting Categorical Data Combinations. "],["probability-and-probability-distributions.html", "Unit 2 Probability and Probability Distributions 2.1 Defining Probability 2.2 Homework 1 2.3 Independence 2.4 Random Variables 2.5 Homework 2 2.6 PMFs and PDFs 2.7 The Uniform Distribution 2.8 The Binomial Distribution 2.9 Homework 3", " Unit 2 Probability and Probability Distributions 2.1 Defining Probability Probability The measure of how likely something is to occur To begin talking about this topic, three definitions will be introduced: experiment - any process, real or hypothetical, in which the possible outcomes can be identified ahead of time10 event - a well-defined set of possible outcomes of the experiment11 sample space - a collection or a set of possible outcomes or results of a random experiment The simplest way of calculating the probability that an event \\(E\\) occurs is: \\[P(E) = \\frac{\\text{number of desired outcomes}}{\\text{total possible outcomes}}\\] 2.2 Homework 1 The concept of probability can be better illustrated through examples. Below are some situations where probability is of interest. For each example, identify the experiment, the event and calculate the probability. What is the probability that a coin toss comes up heads? When a die is rolled, what is the probability of getting an even number? When two dice are rolled, what is the probability of rolling a double? In a standard deck of 52 cards, what is the probability of drawing a face card? In a drawer containing 3 pairs of black socks and 4 pairs of white socks, what is the probability of drawing a black pair? 2.3 Independence In talking about probability, independence is a term that often comes up. In statistics, two events \\(A\\) and \\(B\\) are said to be independent if the occurrence of one does not affect the probability of occurrence of the other12. Here are some examples of independent events: Flipping a fair coin and rolling a die Drawing a card from a deck, putting it back, and drawing another card Another related concept is that of disjoint events. Disjoint, or mutually exclusive events, are events that cannot occur at the same time. If two events \\(A\\) and \\(B\\) are disjoint, the occurrence of \\(A\\) means that \\(B\\) cannot occur and vice versa. Examples: Getting both heads and tails in one coin flip Having one pet that is both a dog and a bird (considering only real species) 2.4 Random Variables Random variables are very important building blocks of probability theory, but this course only requires an informal definition. Here and in the next chapters, mathematical underpinnings have been excluded where permissible to keep the content easily understandable. What is a random variable? It is a variable whose values depend on the outcome of some experiment.13 This informal definition combines two important terms previously introduced - variable and experiment. In statistical equations, random variables are typically represented by capital letters. Random variables are best described by probability distributions. Again, building off of previously defined terms, probability distributions are, simply put, distributions of probability. More precisely, a probability distribution is a mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment14. 2.4.1 Discrete and Continuous Random variables can either be discrete or continuous. This classification pertains to the types of values a random variable can take. Discrete random variables are those that have distinct, countable values. Continuous random variables, on the other hand, take on any value within an interval, i.e. infinitely many values. As with everything else, this can be better understood with examples. Below is an exercise that will require the application of the conceptual definitions stated above. 2.5 Homework 2 Classify each random variable as discrete or continuous. \\(X\\), a random variable that has a value of 1 if a coin toss comes up heads, 0 otherwise A random variable \\(Y\\) which takes on the number of times an even number comes up in 10 rolls of a die A random variable \\(C\\) which is the exact temperature in a certain room at 10:20:00 AM \\(M\\), the exact mass of a randomly selected person in the class \\(K\\), the mass of a randomly selected person in a mall rounded to the nearest gram 2.6 PMFs and PDFs The probability mass function or pmf is a function that gives the probability that a discrete random variable is exactly equal to some value. The continuous analog of pmfs are pdfs or probability density functions. 2.7 The Uniform Distribution The uniform distribution has two forms - discrete and continuous. As the name implies, the uniform distribution describes an experiment where all possible outcomes in the sample space are equally likely to occur. To differentiate between the discrete uniform and continuous uniform distributions, it is important to look at what kind of experiment is generating the data and what kind of values are possible given that process. Suppose we are drawing a random card from a deck of 52. This is a discrete process because we can count the number of possible outcomes. Since each card has an equal chance of being drawn, a random variable \\(R\\) corresponding to the card that will be drawn is uniformly distributed, with pmf given by: \\[p(x) = \\frac{1}{N}, \\hspace{10mm} N = 1, 2, 3, \\dotsc\\] \\(N\\) is the total number of possible outcomes. The continuous uniform distribution, on the other hand, can be illustrated by considering an interval on the number line, say \\((1,5)\\). consider a random variable \\(B\\) that is uniformly distributed in this interval. This means that \\(B\\) is equally likely to take on any value between 1 and 5, including fractions and irrational numbers. The pdf of the continuous uniform distribution is given by \\[f(x) = \\frac{1}{b - a}, \\hspace{10mm} a &lt; x &lt; b\\] where \\(a\\) and \\(b\\) are the bounds of the interval. Note that the equation above is not dependent on \\(x\\), i.e. regardless of the value of the random variable, the probability will be the same. When a random variable is uniformly distributed across an interval, each and every value in the interval is equally likely to occur. In the example, every number in the interval \\((1, 5)\\) has an equal chance of being chosen. Suppose a number is selected at random from this interval. What is the probability that the number chosen will be between 2 and 4? Let \\(i\\) and \\(j\\) be the bounds of the desired interval. \\[ \\begin{align} P(E) &amp;= (j - i) \\cdot \\frac{1}{b-a} \\\\ &amp;= (4 - 2) \\cdot \\frac{1}{5 - 1} \\\\ &amp;= 2 \\cdot \\frac{1}{4} = \\frac{1}{2} \\end{align} \\] 2.8 The Binomial Distribution 2.8.1 Bernoulli Trial A Bernoulli trial is an experiment that has only two possible outcomes - success and failure. A success is denoted by 1 and a failure by 0. Let \\(\\pi\\) be the probability of success, \\(0 &lt; \\pi &lt; 1\\). This is perhaps the simplest experiment15, but gives rise to an entire family of discrete distributions. Of these, the most common is the BINOMIAL DISTRIBUTION whose probability mass function is introduced below. 2.8.2 PMF of a Binomial Distribution16 Let \\(X\\) be the number of 1s or successes in \\(n\\) independently performed Bernoulli trials. \\(X\\) then has a binomial distribution, denoted by \\(X \\sim \\text{Bi}(n, \\pi)\\), and with a pmf given by: \\[p(k) = {n \\choose k} \\pi^k(1 - \\pi)^{n-k}, \\hspace{10mm} k =0, 1, 2, \\dotsc, n.\\] The parameters are \\(n\\), the total number of trials, and \\(\\pi\\) which represents the probability of success. \\(k\\) can be thought of as the value of the random variable. As an illustrative example, consider a coin toss. There are only two possible outcomes - heads or tails. This can be considered a Bernoulli trial where success is defined as getting heads and failure is getting tails. To calculate the probability of success, the basic definition of probability introduced at the beginning of this unit can be used. \\[ \\begin{align} P(E) &amp;= \\frac{\\text{number of desired outcomes}}{\\text{total possible outcomes}} \\\\ P(\\text{heads}) &amp;= \\frac{1}{2} = 0.5 = \\pi \\end{align} \\] Suppose that the number of heads in 5 coin tosses is of interest. This random variable \\(X\\) fits the definition of a binomially distributed random variable, \\(X \\sim \\text{Bi}(5, 0.5)\\). What is the probability of getting 2 heads in 5 coin tosses? To answer the question above, the given simply need to be substituted into the pmf of the binomial distribution given earlier: \\[ \\begin{align} p(k) &amp;= {n \\choose k} \\pi^k (1 - \\pi)^{n-k} \\\\ p(2) &amp;= {5 \\choose 2} (0.5)^2 (1 - 0.5)^{5 - 2} \\\\ p(2) &amp;= {5 \\choose 2} (0.5)^2 (0.5)^3 = 0.3125 \\end{align} \\] 2.9 Homework 3 Read about skewness in distributions and define the following terms: left-skewed right-skewed Highlight the differences between the two. DeGroot, M. H., &amp; Schervish, M. J. (2012). Probability and Statistics. Pearson Education. DeGroot, M. H., &amp; Schervish, M. J. (2012). Probability and Statistics. Pearson Education. Independence. Random variable. Probability distribution. Bataller, R (2013). Elementary Probability Theory for Math/AMF Majors. Mathematics Department, School of Science and Engineering, Ateneo de Manila University. Bataller, R (2013). Elementary Probability Theory for Math/AMF Majors. Mathematics Department, School of Science and Engineering, Ateneo de Manila University. "],["the-normal-distribution-and-the-central-limit-theorem.html", "Unit 3 The Normal Distribution and the Central Limit Theorem 3.1 Population vs. Sample 3.2 The Normal Distribution 3.3 The Empirical Rule 3.4 The Central Limit Theorem 3.5 Concluding Remark", " Unit 3 The Normal Distribution and the Central Limit Theorem 3.1 Population vs. Sample Population - all the elements from a set of data Sample - one or more observations taken from the population A quantity taken for the entire population is known as a population parameter, while that taken for a sample is called a sample statistic. 3.1.1 Why do we take samples? Collecting data from an entire population is expensive, time-consuming, and most of the time, impossible! If all the data were needed to make even the simplest of inferences, analyses would be severely constrained. Most of the time, only a snapshot of the data, a sample, is made available. Statistics allows one to transcend the limits of data collection and enables one to make inferences about the entire population based only on a sample. Sampling Distribution The sampling distribution of a statistic is a probability distribution based on a large number of samples of size \\(n\\) from a given population.17 This means that The sampling distribution is a probability distribution. From the population, many samples of the same size are taken and a statistic (e.g mean, proportion) is taken for each sample. Using the values of the statistic that were calculated from the samples, a distribution is then obtained. Standard Error The measure of the variability of the sample means \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\), \\(\\sigma\\) is the population standard deviation 3.2 The Normal Distribution The normal distribution is the most common among all probability distributions, perhaps because it describes a lot of variables quite well. According to an introductory statistics book, Many variables are nearly normal, but none are exactly normal. Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems.18 The normal distribution is bell-shaped. It is symmetric and unimodal; it has one peak and tapers off at both ends in exactly the same way. Below is an example of a normal distribution with a mean of 2 and a standard deviation of 1. Two parameters are used to describe the normal distribution - mean \\(\\mu\\) and standard deviation \\(\\sigma\\). A mean of 0 and a standard deviation of 1 corresponds to the standard normal distribution which is shown below: 3.2.1 PDF of a Normal Distribution The normal distribution is continuous, i.e. a normally distributed random variable can take on any value between \\((-\\infty, +\\infty)\\). The pdf for the normal distribution is: \\[f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp{\\left[-\\frac{1}{2} \\cdot \\left( \\frac{x - \\mu}{\\sigma}\\right)^2\\right]}\\] 3.2.2 The Z-score measures how many standard deviations above or below the mean a data point is Formula for the z-score: \\[Z = \\frac{X-\\mu}{\\sigma}, \\hspace{10mm} X \\sim N(\\mu, \\sigma)\\] Obtaining a variable \\(Z\\) from a normally distributed random variable \\(X\\) is called standardization, and the variable \\(Z\\) is called a standard normal variable. The above equation looks like the last part of the normal pdf. Substituting this expression for z yields: \\[f(z) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\exp{\\left[-\\frac{1}{2} \\cdot z^2\\right]}\\] This is the equation for the standard normal distribution. These equations are rarely used to compute probabilities because various software have made these available through the use of functions. For example, the Excel function NORMDIST()19 gives the exact value of the pdf for any normal distribution specified by a mean and standard deviation. However, introducing these concepts is still important to justify the robustness of the statistical tools that will be discussed later on. Knowing these fundamental concepts allows one to confidently use and interpret the results of many different methods such as hypothesis testing, linear regression, etc. 3.3 The Empirical Rule Another useful property of normally distributed data is given by the empirical rule. Given that the distribution of the data is bell-shaped, this rule states that: Approximately 68% of the data lie within 1 standard deviation from the mean Approximately 95% of the data, 2 standard deviations About 99.7% of the data, 3 standard deviations Figure 3.1: The Empirical Rule As an example, suppose that for the entire population of a certain country, age is normally distributed with a mean of 37 years and standard deviation of 3.5 years. According to the Empirical Rule, 68% of this population have ages that lie within 1 standard deviation from the mean, which is 37. Within 1 standard deviation means 1 \\(\\sigma\\) above and below the mean. To compute the age interval where 68% of the population lie: \\[\\begin{align} \\text{lower bound} &amp;= \\overline{X} - \\sigma \\\\ &amp;= 37 - 3.5 \\\\ &amp;= 33.5 \\\\ \\text{upper bound} &amp;= \\overline{X} + \\sigma \\\\ &amp;= 37 + 3.5 \\\\ &amp;= 40.5 \\end{align}\\] Hence, the interval is \\((33.5, 40.5)\\). 3.4 The Central Limit Theorem 3.4.1 Conditions for the CLT The Central Limit Theorem (CLT) states that for a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), taking sufficiently large random samples with replacement and computing sample means will yield a distribution of sample means (sampling distribution) that is approximately normal.20 The CLT holds true provided that the following conditions are met: Independence: The sampled observations must be independent. Sample size/skew: If the population is skewed, the sample size \\(n\\) must be greater than 30. If not, the population distribution must be normal. Under this theorem, the parameters for the sampling distribution are \\(\\mu\\), the population mean, and \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\), the standard error. That is, \\(\\overline{X} \\sim N(\\text{mean} = \\mu, \\sigma = \\text{SE})\\). 3.4.2 Applying the CLT21 Suppose my iPod has 3,000 songs. I know that the distribution of lengths of these songs is right-skewed, and for this iPod, the mean length is 3.45 minutes and the standard deviation is 1.63 minutes. Im about to take a trip to visit my parents and the drive is 6 hours. I make a random playlist of 100 songs. What is the probability that my playlist lasts the entire drive? Given: \\(\\mu\\) = 3.45 minutes \\(\\sigma\\) = 1.63 minutes 6 hours = 360 minutes Find: probability that 100 randomly selected songs lasts 360 minutes, which is the same as probability that the average length of the 100 randomly selected songs is at least \\(360/100 = 3.6\\) minutes \\(P(\\overline{X} \\geq 3.6)\\) According to the CLT: \\[ \\overline{X} \\sim N(\\mu = 3.45, ~ \\text{SE} = \\frac{1.63}{\\sqrt{100}} = 0.163) \\] \\[Z = \\frac{x - \\mu}{\\sigma} = \\frac{3.6 - 3.45}{0.163} = 0.92\\] \\[P(Z \\geq 0.92) = 0.1788\\] 3.5 Concluding Remark This unit has introduced one of the most important theorems in statistics and in doing so, has inevitably scratched the surface of one of the most fundamental inferential methods - hypothesis testing. The next unit will more formally introduce the rudiments of hypothesis testing - from setting up hypotheses to selecting the appropriate confidence level. After establishing the fundamental definitions, applications and examples will be presented which will hopefully solidify the previous discussions. The normal distribution will be re-introduced and in the context of hypothesis testing, its ubiquity in the realm of inferential statistics will be more apparent. Sampling Distributions. Diez, D., Centinkaya-Rundel, M., &amp; Barr, C. (2019). OpenIntro Statistics. OpenIntro. NORMDIST Function. Central Limit Theorem. Inferential Statistics, Coursera. "],["hypothesis-testing.html", "Unit 4 Hypothesis Testing 4.1 Basic Concepts 4.2 Simple and Composite Hypotheses 4.3 Testing Hypotheses", " Unit 4 Hypothesis Testing 4.1 Basic Concepts Suppose one is interested in finding out whether one advertising method, say X, is more effective than a similar method Y in increasing the sales of a business. Is method X more effective than method Y? A simple way to answer this is to collect data for both Method X and Method Y. Suppose we randomly select a day or week, and count the number of sales attributable to each method during this period. We could then get the average of both methods and take the difference. We can say that Method X is better if the difference is positive, or worse if the difference is negative. This seems simple, but remember that the values will vary each time the data is collected. Hence, the difference in the averages will also be different and we cannot be 100% certain of the magnitude and direction of the difference. With the uncertainties that are inherent in doing an experiment, how then can answer such questions as the one above in a principled way? The answer is Statistics. Hypothesis testing is a tool used to obtain statistical evidence to arrive at certain decisions given the data, accounting for uncertainty. 4.1.1 The Null and Alternative Hypotheses Suppose we have a space \\(\\Omega\\) where all possible parameters \\(\\theta\\) about the data can be found. We can then divide this set into two mutually exclusive sets which we call \\(\\Omega_0\\) and \\(\\Omega_1\\). We shall then denote \\(H_0\\) as the hypothesis that \\(\\theta \\in \\Omega_0\\), while \\(H_1\\) is the hypothesis when \\(\\theta \\in \\Omega_1\\). In statistical literature, these two hypotheses are called the null hypothesis and the alternative hypothesis, respectively. 4.1.2 Decision Errors Since there are two hypotheses and they are disjoint, only one hypothesis can be true. If the wrong hypothesis is taken to be true, a loss or cost is incurred. Suppose that the \\(\\theta \\in \\Omega_0\\) or the null hypothesis is true, but it is rejected. This is called a Type I error. If instead \\(\\theta \\in \\Omega_1\\) or the alternative hypothesis is true, but we do not reject the null hypothesis, we are making a Type II error. For most cases, we can set our hypothesis test to have a certain Type I error rate, \\(\\alpha\\) (a number from 0 to 1), which corresponds to the probability of committing a Type I error. Thus, if we have a test with \\(\\alpha = 0.05\\), it means that we have at most a 5% chance of rejecting the null hypothesis when the null hypothesis is true. The Type II error, which is denoted by \\(\\beta\\) and corresponds to not rejecting a false null hypothesis, is related to a concept called power. Statistical power is the probability that the hypothesis test will find an effect if there is an effect to be found. Mathematically, power is computed as \\(1 - \\beta\\). Experimental design includes what is called power analysis, which determines the appropriate sample size needed to detect the desired difference or effect. This is beyond the scope of the course, and will not be discussed further here. 4.1.3 Setting Up Hypotheses Using the advertising example above, suppose that the hypothesis is \\(\\mu_X &gt; \\mu_Y\\) with respect to sales, i.e. Method X generates more sales that Method Y. Analysts could claim that the data supports the theory that \\(\\mu_X &gt; \\mu_Y\\), when in fact \\(\\mu_X \\leq \\mu_Y\\) (Case 1). They could also mistakenly claim that data fails to support \\(\\mu_X\\) &gt; $_Y when it is true (Case 2). Both cases entail a rejection of the null hypothesis and for both situations, the setup of the null and alternative hypothesis are different, as shown below. For Case 1: \\[ \\begin{aligned} H_0 &amp;: \\mu_{X} \\leq \\mu_{Y}\\\\ H_1 &amp;: \\mu_{X} &gt; \\mu_{Y} \\end{aligned} \\] For Case 2: \\[ \\begin{aligned} H_0 &amp;: \\mu_{X} \\geq \\mu_{Y}\\\\ H_1 &amp;: \\mu_{X} &lt; \\mu_{Y} \\end{aligned} \\] It is helpful to think about the consequence of mistakenly rejecting the null hypothesis, i.e., committing a type I error. In most business cases, it is costly to introduce a new method as it will likely entail higher costs during implementation than retaining the old method. Thus, mistakenly rejecting the null hypothesis of Case 1 will be more costly to the business than 2 as Case 2 is about retaining the current strategy. Intuitively, \\(H_0\\) represents the status quo or current situation (no difference, hence the equality) and \\(H_1\\) asserts that there is a difference. This is why in practice, \\(H_0\\) must always contain some form of equality (\\(=\\), \\(\\leq\\), \\(\\geq\\)) and \\(H_1\\) must be stated in a way that complements \\(H_0\\) exactly. 4.2 Simple and Composite Hypotheses A simple hypothesis is where the parameter \\(\\theta\\) has only one value in either \\(\\Omega_0\\) or \\(\\Omega_1\\). Such a setup is shown below: \\[ \\begin{aligned} H_0 &amp;: \\theta=\\theta_0 \\\\ H_1&amp;: \\theta \\neq \\theta_0 \\end{aligned} \\] \\(\\theta_0\\) is the parameter value of the null hypothesis set. Statistical tests that seek to test this hypothesis setup are called two-sided hypothesis tests. A composite hypothesis, on the other hand, is a setup where the hypothesis space (either \\(\\Omega_0\\) or \\(\\Omega_1\\)) contains more than one value for \\(\\theta_0\\). There are two ways to set up a composite hypothesis and the difference lies in the inequality sign used. One could either do: \\[ \\begin{aligned} H_0 &amp;: \\theta\\leq\\theta_0 \\\\ H_1&amp;: \\theta &gt; \\theta_0 \\end{aligned} \\] or the opposite which is: \\[ \\begin{aligned} H_0 &amp;: \\theta\\geq\\theta_0 \\\\ H_1&amp;: \\theta &lt; \\theta_0 \\end{aligned} \\] Statistical tests for composite hypotheses are called one-sided tests and, depending on the inequality sign of the alternative hypothesis, it can either be called a right-tailed (first) or a left-tailed test (second). 4.3 Testing Hypotheses 4.3.1 Using a Critical Value \\(c\\) In doing a hypothesis test, we either decide to reject or not reject the null hypothesis. To do so, we need to define a statistic \\(T\\) as the distance between the sample statistic and \\(\\theta_0\\). \\(T\\) is thus random (as our data is a random sample of our population). Given a particular value of \\(\\theta_0\\), we might want to have a threshold \\(c\\), which we will call the critical value. Using \\(c\\) we can decide to reject \\(H_0\\) if \\(T \\geq c\\), or not reject \\(H_0\\) if \\(T &lt; c\\). Each threshold corresponds to a value of \\(\\alpha\\). The correct way of testing hypotheses is to set the \\(\\alpha\\) first and then use the corresponding critical value \\(c\\). The mathematics behind this is ommitted for now as it can get quite overwhelming. If this has piqued curiosity, however, a number of introductory statistics texts and resources on the Internet explore the mathematical underpinnings of hypothesis testing in detail. The applications of this concept will be shown in the unit on z-tests and t-tests. 4.3.2 Visualizing Rejection Regions Values that are greater than or equal to a certain threshold \\(c\\) that intersect with the alternative hypothesis parameter space \\(\\Omega_1\\) are contained within the rejection region. Rejection regions in the visualizations below are colored in blue. For a two-sided hypothesis test, we reject on both tails for a symmetric distribution (usually a normal distribution). This is so since an extremely high or low sample statistic can be evidence for us to reject \\(H_0\\) (as implied by the \\(\\neq\\) sign). Since the probability of rejecting a true \\(H_0\\) supposedly covers both high and low values, the critical value \\(c\\) to be used should correspond to \\(\\alpha/2\\), not \\(\\alpha\\). For one-sided tests, we reject depending on the parameter space of the alternate hypothesis. Hence, the rejection region for a left-tailed test can be shown as: The rejection region for a right-tailed test is shown below: 4.3.3 The p-value Another important concept is the p-value which is often reported in analyses to denote statistical significance relative to a set \\(\\alpha\\) (usually 0.05). It is defined as the probability of obtaining test results at least as extreme as the results actually observed when \\(H_0\\) is true. It can be thought of as a measure of how surprised you are with the data. Higher values mean that the data is not at all surprising relative to the null hypothesis. Although this might be the case, p-values do not dictate the probability that the null hypothesis is true given the data, and cannot be used to draw conclusions on how likely the null hypothesis is compared to the alternative, and vice versa. It only allows one to decide whether or not to reject \\(H_0\\). Nonetheless, we can by definition, use p-values to test for hypothesis. This is done by calculating the p-value and comparing it with a set \\(\\alpha\\) level. We reject the null hypothesis if the p-value \\(p \\leq \\alpha\\). "],["z-tests-t-tests-and-confidence-intervals.html", "Unit 5 Z-tests, t-tests, and Confidence Intervals 5.1 Starting with Z 5.2 Doing a Z-test 5.3 Students t-distribution 5.4 A Simple t-test Example 5.5 Tests for Composite Hypotheses 5.6 A Two-Sample t-test 5.7 Hypothesis Testing Examples 5.8 Confidence Intervals", " Unit 5 Z-tests, t-tests, and Confidence Intervals .prob { color: gray; font-size:110%; border: 2px solid gray; border-radius: 5px; padding-left: 20px; padding-right: 20px; padding-top: 10px; padding-bottom: 10px; } 5.1 Starting with Z The standard normal distribution, as discussed in Unit 3, is also called the Z Distribution because the process of standardization yields a random variable commonly called \\(Z\\). 5.2 Doing a Z-test Now that we have been introduced to the Z Distribution and the hypothesis testing framework, we are now ready to answer questions like, Is there evidence to conclude that the mean of a population is equal to a certain number? 5.2.1 Using the Z Statistic Suppose a pasta company claims that the net weight of one pack of pasta is 100 g, with a standard deviation of 0.5 g. You are hired by this company to do statistical analysis for them, specifically to test whether the 1 million packs of pasta produced this week meet their 100-gram claim. You cannot weigh all the 1 million packs individually because reopening them would cost the company money, and it would obviously take you a very long time to weigh each pack. The manager of the manufacturing division gives you 500 packs of pasta to work with and hopes that with your statistical knowledge, you will be able to prove or dispute their claim. What should you do? The problem above can be solved using a simple hypothesis setup: \\[ \\begin{aligned} H_0 &amp;: \\mu = \\mu_0 \\\\ H_1 &amp;: \\mu \\neq \\mu_0 \\end{aligned} \\] where \\(\\mu\\) is the true mean of the population, and \\(\\mu_0\\) is the reference mean. In the problem, the reference mean is 100 g, since that is what the company claims. The population, whose true mean is \\(\\mu\\), is the batch of 1 million packs produced during the week. Using statistical jargon, we would like to test whether the population mean is indeed 100 g using the sample of 500 pasta packs. Recall that the sample mean, \\(\\overline{X}\\), is an estimate of \\(\\mu\\). Hence, taking the mean of the 500 packs of pasta would give an estimate of the true mean weight of the batch produced. \\(\\overline{X}\\) is simply the arithmetic average of the weights of the 500 packs, i.e., \\(\\overline{X} = \\sum_i^{n}\\frac{x_i}{n}\\). To test this hypothesis, we can take the difference between \\(\\overline{X}\\) and \\(\\mu_0\\) and check if it is large enough to say that the population mean \\(\\mu\\) is not 100 g. Note that the direction of the difference (i.e. whether it is positive or negative) does not matter as implied by the hypotheses. Thus, from Unit 4, we can reject the null hypothesis when: \\[|\\overline{X} - \\mu_0| \\geq c\\] where \\(c\\) is the critical value. At this point, one might be tempted to choose an arbitrary value for \\(c\\), say 10 or 15. That would undermine the integrity of the procedure performed. Recall from the previous discussion that the appropriate \\(c\\) is determined by first setting an \\(\\alpha\\). For this problem, assume that the company would like to be 95% sure that the batch produced meets their 100-gram claim. Given this 95% confidence level, \\(\\alpha\\) would have a value of 0.05 since we can only allow incorrect rejection of \\(H_0\\) 5% of the time. This is a two-tailed test and hence, the critical value to be used should correspond to \\(\\alpha/2\\). What is the appropriate critical value to use? We have 500 samples, a sufficiently large number, and it would be safe to say that the samples are independent. Since the assumptions are met, we can use the Central Limit Theorem and assume that \\(\\overline{X}\\) is normally distributed. All that needs to be done is standardize \\(\\overline{X}\\) to get the corresponding Z statistic and compare that with the critical Z-score. To standardize, the formula for the Z-score from Unit 3 is used. Note that this time, the variable to be standardized is \\(\\overline{X}\\), not \\(X\\), and hence the appropriate mean and standard deviation must be used. We know that \\(\\overline{X} \\sim N(\\mu, SE = \\sigma/\\sqrt{n})\\) and so: \\[ \\begin{align} Z &amp;= \\frac{\\overline{X} - \\mu}{SE} \\\\ &amp;= \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\\\ &amp;= \\frac{\\sqrt{n} \\cdot \\big(\\overline{X} - \\mu\\big)}{\\sigma} \\end{align} \\] To find the critical Z-score corresponding to \\(\\alpha/2\\), one would need to look at the cumulative distribution function or CDF of the Z distribution. In general, the CDF gives the probability that a random variable is less than or equal to a certain value. Formally, the critical Z-score is computed as \\[c = \\Phi^{-1}(1-\\alpha_0/2) \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] where \\(\\Phi\\) is the CDF of the standard normal distribution. Thankfully, because the standard normal distribution is commonly used, Z tables and computers have made this much easier. For the pasta problem, the Z-score for a two-tailed test corresponding to \\(\\alpha = 0.05\\) is 1.96. Therefore, we reject the null hypothesis if the calculated Z statistic, \\(Z\\), is \\(&lt; -1.96\\) or \\(&gt; 1.96\\), which corresponds to the shaded regions below: 5.2.2 p-values P-values can also be used to decide whether or not the null hypothesis should be rejected. This can be done by using the standard normal distribution to get the cumulative probability, \\(\\Phi(Z)\\), which corresponds to the \\(Z\\) statistic we have computed from \\(\\overline{X}\\). We reject the null hypothesis if \\(\\alpha \\leq 1-\\Phi(Z)\\). 5.3 Students t-distribution The Z-test assumes that the true variance of the population is known, which is not always the case. This renders the Z-test unusable. For these situations where the variance is unknown, we use the t-test, which is based on the t-distribution. The t-distribution, originally called Students t-distribution (from the pseudonym Student of the inventor, William Sealy Gosset), has an extra parameter that describes the shape of the distribution. This parameter is called degrees of freedom, \\(\\nu\\). The graph below shows how different values of \\(\\nu\\) affects the shape of the distribution. 5.4 A Simple t-test Example Just like the z-test, we can also test if the mean of a sample is significantly different from the population mean. For example, for a two-tailed setup like: \\[ \\begin{aligned} H_0 &amp;: \\mu = \\mu_0 \\\\ H_1 &amp;: \\mu \\neq \\mu_0 \\end{aligned} \\] We can then calculate for the variable \\(U\\): \\[U=\\sqrt{n} \\cdot \\frac{|\\overline{X} - \\mu_0|}{\\hat{\\sigma}}\\] Notice that there is now a new variable, \\(\\hat{\\sigma}\\). This represents the sample standard deviation. In previous sections, the sample standard deviation was represented as \\(s\\), and these two notations can be used interchangeably. In general, the caret (\\(\\hat{}\\)) is used to denote an estimate of a population parameter, which in this context is usually the sample statistic. The sample standard deviation \\(\\hat{\\sigma}\\) can be calculated as: \\[\\hat{\\sigma}=\\sqrt{\\frac{\\sum_i^n{(X_i -\\overline{X})^2}}{n-1}}\\] Like the Z-test, we reject \\(H_0\\) if the statistic \\(U\\) is greater than or equal to a critical statistic \\(c\\), i.e. when \\(U \\geq c\\) given that \\(\\mu = \\mu_0\\). The critical statistic \\(c\\) is computed by \\(T^{-1}_{n-1}(1-\\alpha/2)\\) (for a two-tailed test) where \\(T_{n-1}\\) is the cumulative density function of the \\(t\\)-distribution for \\(n-1\\) degrees of freedom. Below is the \\(t\\)-distribution with 5 degrees of freedom: 5.5 Tests for Composite Hypotheses 5.5.1 Rejection Regions In obtaining the \\(Z\\)- or \\(t\\)-statistic for a composite hypothesis setup, the following formula is used: \\[Z = \\sqrt{n} \\cdot \\frac{(\\overline{X} - \\mu_0)}{\\sigma}, \\hspace{10mm} U = \\sqrt{n} \\cdot \\frac{(\\overline{X} - \\mu_0)}{S_X}\\] Comparing this with the equation for a simple hypothesis setup, notice that the only change here is the absence of the absolute value symbol. Because this is a one-tailed test, we now care about the direction of statistic. For a left-tailed test, we reject \\(H_0\\) if the test statistic \\(U \\leq c\\), while for a right-tailed test, we reject \\(H_0\\) if \\(U \\geq c\\). Figure 5.1: Rejection Region for a Left-tailed Test Figure 5.2: Rejection Region for a Right-tailed Test 5.5.2 Calculating p-values For the one-tailed test, we calculate the p-values based on the alternative hypothesis. For example, if we have \\(H_1:\\mu &lt; \\mu_0\\), then we are to find: \\[ \\begin{aligned} \\phi(Z) \\quad &amp;\\text{(for z-test)} \\\\ T_{n-1}(U)\\quad &amp;\\text{(for t-test)} \\end{aligned} \\] On the other hand, for \\(H_1: \\mu &gt; \\mu_0\\): \\[ \\begin{aligned} 1-\\phi(Z) \\quad &amp;\\text{(for z-test)} \\\\ 1-T_{n-1}(U)\\quad &amp;\\text{(for t-test)} \\end{aligned} \\] Note that \\(\\Phi\\) and \\(T\\) are the cumulative distribution functions or CDFs, and they always give the areas of the left tail. Visualizing the rejection regions will explain why we subract the area from 1 in a right-tailed test. 5.6 A Two-Sample t-test There are situations when we want to compare two groups and test if their population means are different, or if one is greater than (or less than) relative to the other. In other words, instead of comparing \\(\\mu\\) to a reference value \\(\\mu_0\\), we are now comparing two population means - \\(\\mu_X\\) and \\(\\mu_Y\\). For a two-sampled test, we now have a new way of calculating the t-statistic. incorporating the new variables \\(\\overline{Y}\\) and \\(\\hat{\\sigma_Y}\\). Below is the formal definition of a two-sample t-statistic: \\[ \\begin{aligned} U = \\frac{(m+n-2)^{1/2}(\\overline{X} - \\overline{Y})}{(\\frac{1}{m} + \\frac{1}{n})^{1/2}(S^2_X+S^2_Y)^{1/2}} \\end{aligned} \\] \\(\\overline{X}\\) and \\(\\overline{Y}\\) are the respective sample means and \\(S_X^2\\) and \\(S_Y^2\\) are the residual sum of squares or RSS, defined as \\(S_X^2 = \\sum_i^m(X_i - \\overline{X})^2\\) and \\(S^2_Y = \\sum_j^n(Y_j - \\overline{Y})^2\\). We use the t-distribution with \\(m + n - 2\\) degrees of freedom for inference in this case. 5.7 Hypothesis Testing Examples 5.7.1 Problem 1 The manufacturer of a certain automobile claims that under typical urban driving conditions, the automobile will travel on average at least 20 miles per gallon of gasoline. An owner of one of these automobile notes the mileages that she has obtained in her own urban driving when she fills her automobiles tank with gasoline on nine different occasions. Her records show the following, in miles per gallon: 15.6, 18.6, 18.3, 20.1, 21.5, 18.4, 19.1, 20.4, and 19.0. Test the manufacturers claim by carrying out a test at a significance level \\(\\alpha\\) = 0.05. To solve this problem, we first need to formulate the hypothesis that is being asked. Since the manufacturer claims that the car runs on at least 20 miles per gallon, we can say that the null and alternative hypotheses are: \\[ \\begin{aligned} H_0 &amp;: \\mu \\geq 20 \\\\ H_1 &amp;: \\mu &lt; 20 \\end{aligned} \\] We do not have information about the population variance and thus we use a t-test. The next step would be to calculate the t-statistic. The sample mean in this case is simply: \\[ \\begin{aligned} \\overline{X} &amp;= \\frac{\\sum^{n}_{i}X_{i}}{n}\\\\ &amp;= \\frac{(15.6 + 18.6 + 18.3 + 20.1 + 21.5 + 18.4 + 19.1 + 20.4 + 19.0)}{9} \\\\ &amp;= 19 \\end{aligned} \\] The sample standard deviation is calculated as: \\[ \\begin{aligned} \\hat{\\sigma} &amp;= \\frac{\\sum_{i}^{n}(X_i - \\overline{X})^2}{n-1}\\\\ &amp;= \\frac{\\sum_{i}^{n}(X_i-19)^2}{n-1} \\\\ &amp;= 1.66 \\end{aligned} \\] Putting this all together, we calculate the t-statistic to be: \\[ \\begin{aligned} U &amp;= \\frac{\\sqrt{n} \\cdot (\\overline{X} - \\mu_0)}{S_X} \\\\ &amp;= \\frac{\\sqrt{9} \\cdot (19 - 20)}{1.66} \\\\ &amp;= -1.81 \\end{aligned} \\] We can decide whether or not to reject the null hypothesis by either calculating the critical statistic or the p-value. 5.7.1.1 Critical Statistic The critical statistic is the t-statistic that corresponds to \\(1 - \\alpha\\), that is, getting the value of \\(T^{-1}_{n-1}(1-\\alpha)\\) where \\(T^{-1}_{n-1}\\) is the inverse of the CDF with \\(n-1\\) degrees of freedom. The degrees of freedom here is \\(\\nu = n-1 = 8\\). In Microsoft Excel this can be done using the function T.INV with arguments probability and deg_freedom. For probability \\(alpha_0 = 0.05\\), and 8 degrees of freedom, we get \\(c = -1.86\\). Since \\(U = -1.81 &gt; -1.86\\), we do not reject the null hypothesis and conclude that the evidence gathered by the owner supports the manufacturers claim that the automobile runs at 20 mpg on average. 5.7.1.2 Calculating the p-value We get the p-value by using the CDF of the t-distribution \\(T_{n-1}(U)\\). This can achieved in Microsoft Excel by calling the function T.DIST with arguments X, which is the statistic for which we evaluate the CDF (which is \\(U\\) in this case), deg_freedom, and cumulative which tells the function if we want to add up the densities of the distribution. Since the function is by default left-tailed, the densities are added up from left to right. This is shown below: Since we want to test if there is enough evidence to reject \\(\\mu \\geq 20\\), it makes sense to get the cumulative probability from the left. It would mean that highly negative values for the statistic are highly unlikely since going from left to right correspond to increasing probabilities. For this problem, we calculate \\(p = 1 - T_{n-1}(U) = 0.054\\). Since \\(p &gt; \\alpha_0\\) we do not reject the null hypothesis at \\(\\alpha_0 = 0.05\\). 5.7.2 Problem 2 As a marketing executive, you are curious if a social media marketing strategy would be more effective than the current paid media marketing strategy of the retail company. To investigate this, you ran two different marketing campaigns alternately for a total of 12 months. The number of sales was recorded each month as shown below: months Marketing Strategies Sales January social media 26 February paid media 77 March social media 16 April paid media 84 May social media 79 June paid media 96 July social media 82 August paid media 14 September social media 50 October paid media 82 November social media 35 December paid media 36 Apply hypothesis testing to find out whether there is evidence that social media marketing is better than paid media marketing in generating sales. We want to find out if social media marketing (S) is better than paid media marketing (P). The hypotheses can be set up as follows: \\[ \\begin{aligned} H_0 &amp;: \\mu_{S} \\leq \\mu_{P} \\\\ H_1 &amp;: \\mu_{S} &gt; \\mu_{P} \\end{aligned} \\] With these statements, the null hypothesis states that social media marketing is at most as good as paid marketing, while \\(H_1\\) states that social media marketing generates more sales. This problem compares two samples, sales due to social media marketing and sales due to paid media marketing. Hence, we use the \\(t\\)-statistic definition for two samples. Below are the sample means and standard deviations for both paid media and social media: Marketing Strategies Mean Standard Deviation Sum of Squares (S) social media 48.0 27.6 3798.0 paid media 64.8 32.2 5196.8 Using this information, we then calculate the \\(t\\)-statistic. Since both marketing strategies were both deployed for an equal number of months during the year, we can say that \\(m = n = 6\\). $$ \\begin{aligned} U &amp;= {( + )(S2_X+S2_Y)^{1/2}} \\ &amp;= {( + )(3,798 + 5,196.8)^{1/2}} \\ &amp;= -0.971 \\end{aligned} $$ Since we are testing for \\(H_0: \\mu_{SM} \\leq \\mu_{PM}\\), we reject it if the statistic \\(U \\geq c\\), where \\(c\\) is equal to \\(T^{-1}_{m+n-1}(1-\\alpha_0)\\). For \\(\\text{df} = m + n - 2 = 10\\), the \\(t\\)-statistic at \\(1 - \\alpha\\) is equal to 1.81. Since \\(U &lt; c\\) we do not reject the null hypothesis. To obtain the p-value, we get the cumulative probability from the cdf \\(T_{m+n-2}(U)\\). Looking at \\(H_0\\), we reject positive values and thus it would make sense to get the area of the right-tail, \\(1 - T_{m+n-2}(U)\\), where a lower probability would mean a very unlikely outcome given the null hypothesis. We get this using the Excel formula = 1 - T.DIST(-0.971, 10, TRUE). We calculate the p-value as \\(0.82\\). At significance level \\(\\alpha = 0.05\\), we do not reject the null hypothesis. We visualize this below: 5.8 Confidence Intervals 5.8.1 What are confidence intervals? Confidence intervals give us a range of plausible values for the population parameter based on results from a sample. The conditions for the CLT must also be met for the confidence interval to be valid. 5.8.2 Constructing a Confidence Interval The confidence interval for the mean is computed as: \\[\\bigg(\\overline{X} - c \\cdot \\frac{s}{\\sqrt{n}}, ~ \\overline{X} + c \\cdot \\frac{s}{\\sqrt{n}}\\bigg)\\] \\(\\overline{x}\\) is the sample mean \\(s\\) is the sample standard deviation \\(n\\) is the sample size \\(c\\) is the critical statistic The quantity \\(c \\cdot \\frac{s}{\\sqrt{n}}\\) is called the margin of error. The critical statistic can either be a Z-score (\\(Z\\)) or t-score (\\(U\\)), depending on which distribution is used for inference. This statistic is related to the significance level \\(\\alpha\\) that was previously set. Recall from Unit 4 that \\(\\alpha\\) or the Type I error rate is the probability of rejecting a true \\(H_0\\). Increasing \\(\\alpha\\) means increasing the tolerance for making a wrong decision, and so confidence in the analysis decreases. This is why the confidence level is computed as \\(1 - \\alpha\\). For \\(\\alpha = 0.05\\), the confidence level is \\(1 - 0.05 = 0.95\\) or 95%. In computing a 95% CI, the \\(Z\\)-score corresponding to the confidence level is used. For example, if \\(\\alpha = 0.05\\) and the \\(Z\\)-distribution is used, the critical Z-score which we will denote as \\(z^*\\) is the \\(Z\\)-score corresponding to 0.95. 5.8.3 Interpreting the Confidence Interval As an example, consider the mileage problem from the previous section. The sample mean \\(\\overline{X}\\) was 19 while the sample standard deviation was 1.6583124. The population variance is unknown, so the t-test was used. The critical \\(t\\)-score corresponding to \\(1 - \\alpha = 0.95\\) is 1.86, and the sample size is 9. Using these information, we can construct a 95% confidence interval for the true average MPG of the car: \\[ \\begin{align} &amp; \\bigg(\\overline{X} \\pm c \\cdot \\frac{s}{\\sqrt{n}} \\bigg) \\\\ &amp;= \\bigg(19 \\pm -1.86 \\times \\frac{1.66}{\\sqrt{9}} \\bigg)\\\\ &amp;= (19 \\pm -1.86 \\times 0.5533 ) \\\\ &amp;= (19 \\pm 1.03) \\\\ &amp;= (17.97, ~ 20.03) \\end{align} \\] This 95% confidence interval IS NOT interpreted as: There is a 95% chance that the true mean is in this interval. The correct interpretation of a confidence interval is: 95% of similarly constructed intervals will contain the true population mean. "]]
