---
title: "An Introduction to Statistics"
author: "Mariefel Nicole Deypalan"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
site: bookdown::bookdown_site
documentclass: book
output:
  bookdown::gitbook:
    df_print: paged
github-repo: mndpln/intro-statistics
---

# Z-tests, t-tests, and Confidence Intervals  

<style>
.prob {
  color: gray;
  font-size:110%;
  border: 2px solid gray;
  border-radius: 5px;
  padding-left: 20px;
  padding-right: 20px;
  padding-top: 10px;
  padding-bottom: 10px;
}
</style>

## Starting with Z  

The standard normal distribution, as discussed in Unit 3, is also called the Z Distribution because the process of **standardization** yields a random variable commonly called $Z$.  

```{r}
z <- seq(-3, 3, length = 100)
plot(
  z,
  dnorm(z),
  type = 'l',
  lwd = 3,
  ylab = "density",
  axes = FALSE,
  main = 'Z Distribution (Standard Normal)'
)
box(bty = "l")
axis(2)
axis(1)
```

## Doing a Z-test  

Now that we have been introduced to the Z Distribution and the hypothesis testing framework, we are now ready to answer questions like, **"Is there evidence to conclude that the mean of a population is equal to a certain number?"**  

### Using the Z Statistic  

<div class="prob">  
Suppose a pasta company claims that the net weight of one pack of pasta is 100 g, with a standard deviation of 0.5 g. You are hired by this company to do statistical analysis for them, specifically to test whether the 1 million packs of pasta produced this week meet their 100-gram claim. You cannot weigh all the 1 million packs individually because reopening them would cost the company money, and it would obviously take you a very long time to weigh each pack. The manager of the manufacturing division gives you 500 packs of pasta to work with and hopes that with your statistical knowledge, you will be able to prove or dispute their claim. What should you do?  
</div>  

<br>

The problem above can be solved using a simple hypothesis setup:  

$$
\begin{aligned}
  H_0 &: \mu = \mu_0 \\
  H_1 &: \mu \neq \mu_0
\end{aligned}
$$

where $\mu$ is the true mean of the population, and $\mu_0$ is the reference mean. In the problem, the reference mean is 100 g, since that is what the company claims. The population, whose true mean is $\mu$, is the batch of 1 million packs produced during the week. Using statistical jargon, we would like to test whether the population mean is indeed 100 g using the sample of 500 pasta packs. Recall that the sample mean, $\overline{X}$, is an estimate of $\mu$. Hence, taking the mean of the 500 packs of pasta would give an estimate of the true mean weight of the batch produced. $\overline{X}$ is simply the arithmetic average of the weights of the 500 packs, i.e., $\overline{X} = \sum_i^{n}\frac{x_i}{n}$. To test this hypothesis, we can take the difference between $\overline{X}$ and $\mu_0$ and check if it is large enough to say that the population mean $\mu$ is not 100 g. Note that the direction of the difference (i.e. whether it is positive or negative) does not matter as implied by the hypotheses. Thus, from Unit 4, we can reject the null hypothesis when:  

$$|\overline{X} - \mu_0| \geq c$$

where $c$ is the *critical value*. At this point, one might be tempted to choose an arbitrary value for $c$, say 10 or 15. That would undermine the integrity of the procedure performed. Recall from the previous discussion that the appropriate $c$ is determined by first setting an $\alpha$.  

For this problem, assume that the company would like to be 95% sure that the batch produced meets their 100-gram claim. Given this 95% confidence level, $\alpha$ would have a value of 0.05 since we can only allow incorrect rejection of $H_0$ 5% of the time. This is a two-tailed test and hence, the critical value to be used should correspond to $\alpha/2$.  

What is the appropriate critical value to use? We have 500 samples, a sufficiently large number, and it would be safe to say that the samples are independent. Since the assumptions are met, we can use the **Central Limit Theorem** and assume that $\overline{X}$ is normally distributed. All that needs to be done is **standardize** $\overline{X}$ to get the corresponding Z statistic and compare that with the critical **Z-score**.  

To standardize, the formula for the Z-score from Unit 3 is used. Note that this time, the variable to be standardized is $\overline{X}$, not $X$, and hence the appropriate mean and standard deviation must be used. We know that $\overline{X} \sim N(\mu, SE = \sigma/\sqrt{n})$ and so:  

$$
\begin{align}
Z &= \frac{\overline{X} - \mu}{SE} \\
&= \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \\
&= \frac{\sqrt{n} \cdot \big(\overline{X} - \mu\big)}{\sigma}
\end{align}
$$

To find the critical Z-score corresponding to $\alpha/2$, one would need to look at the **cumulative distribution function** or CDF of the Z distribution. In general, the CDF gives the probability that a random variable is **less than or equal** to a certain value. Formally, the critical Z-score is computed as  

$$c = \Phi^{-1}(1-\alpha_0/2) \cdot \frac{\sigma}{\sqrt{n}}$$  

where $\Phi$ is the CDF of the standard normal distribution. Thankfully, because the standard normal distribution is commonly used, Z tables and computers have made this much easier. For the pasta problem, the Z-score for a two-tailed test corresponding to $\alpha = 0.05$ is 1.96. Therefore, we reject the null hypothesis if the calculated Z statistic, $Z$, is $< -1.96$ or $> 1.96$, which corresponds to the shaded regions below:  

```{r}
q <- qnorm(1 - 0.05 / 2)
shade.dist(
  a = -q,
  b = q,
  type = 'two',
  from = -3,
  to = 3,
  xlab = "Z"
)
abline(v = 1, lty = 2)
abline(v = 2.5, lty = 3)
text(x = 1.5,
     y = 0.3,
     labels = expression(paste("do not\nreject ", H[0])))
text(x = 2.9,
     y = 0.3,
     labels = expression(paste("reject ", H[0])))
title(expression(paste("Critical regions at ", alpha == 0.05)))
```

### p-values  

P-values can also be used to decide whether or not the null hypothesis should be rejected. This can be done by using the standard normal distribution to get the cumulative probability, $\Phi(Z)$, which corresponds to the $Z$ statistic we have computed from $\overline{X}$. We reject the null hypothesis if $\alpha \leq 1-\Phi(Z)$.  

## Student's t-distribution  

The Z-test assumes that the true variance of the population is known, which is not always the case. This renders the Z-test unusable. For these situations where the variance is unknown, we use the t-test, which is based on the t-distribution.  

The t-distribution, originally called Student's t-distribution (from the pseudonym "Student" of the inventor, William Sealy Gosset), has an extra parameter that describes the shape of the distribution. This parameter is called **degrees of freedom**, $\nu$. The graph below shows how different values of $\nu$ affects the shape of the distribution.  

```{r}
df <- c(5, 3, 2, 1)
tx <- seq(-3, 3, length = 100)
cols <- brewer.pal(n = length(df), name = "Set2")
lwd_param <- 2

for (i in 1:length(df)) {
  if (i == 1) {
    plot(
      x = tx,
      y = dt(tx, df = df[i]),
      type = 'l',
      frame.plot = FALSE,
      xlab = "U",
      ylab = "density",
      col = cols[i],
      lwd = lwd_param
    )
    box(bty = "l")
    axis(2)
    axis(1)
  } else {
    lines(
      x = tx,
      y = dt(tx, df = df[i]),
      col = cols[i],
      lwd = lwd_param
    )
  }
}
title("t-distribution by degrees of freedom")
legend(
  "topright",
  legend = paste0("v = ", df),
  lty = 1,
  col = cols,
  bty =  'n'
)
```

<br>

## A Simple t-test Example  

Just like the z-test, we can also test if the mean of a sample is significantly different from the population mean. For example, for a two-tailed setup like:  

$$
\begin{aligned}
  H_0 &: \mu = \mu_0 \\
  H_1 &: \mu \neq \mu_0
\end{aligned}
$$

We can then calculate for the variable $U$:  

$$U=\sqrt{n} \cdot \frac{|\overline{X} - \mu_0|}{\hat{\sigma}}$$  

Notice that there is now a new variable, $\hat{\sigma}$. This represents the sample standard deviation. In previous sections, the sample standard deviation was represented as $s$, and these two notations can be used interchangeably. In general, the caret ($\hat{}$) is used to denote an estimate of a population parameter, which in this context is usually the sample statistic.  

The sample standard deviation $\hat{\sigma}$ can be calculated as:  

$$\hat{\sigma}=\sqrt{\frac{\sum_i^n{(X_i -\overline{X})^2}}{n-1}}$$  

Like the Z-test, we reject $H_0$ if the statistic $U$ is greater than or equal to a critical statistic $c$, i.e. when $U \geq c$ given that $\mu = \mu_0$. The critical statistic $c$ is computed by $T^{-1}_{n-1}(1-\alpha/2)$ (for a two-tailed test) where $T_{n-1}$ is the cumulative density function of the $t$-distribution for $n-1$ degrees of freedom. Below is the $t$-distribution with 5 degrees of freedom:  

```{r}
q <- qt(1 - 0.05 / 2, 5)
shade.dist(
  a = -q,
  b = q,
  param = list(df = 5),
  dist = "dt",
  type = 'two',
  from = -5,
  to = 5,
  xlab = "U"
)
abline(v = 1, lty = 2)
abline(v = 2.7, lty = 3)
text(x = 1.8,
     y = 0.3,
     labels = expression(paste("do not\nreject ", H[0])))
text(x = 3.5,
     y = 0.3,
     labels = expression(paste("reject ", H[0])))
title(expression(paste(
  "Critical regions at ", alpha == 0.05, ", ", nu == 5
)))
```

<br>

## Tests for Composite Hypotheses  

### Rejection Regions  

In obtaining the Z- or t-statistic for a composite hypothesis setup, the following formula is used:  

$$Z = \frac{\sqrt{n} \cdot (\overline{X} - \mu_0)}{\sigma}, \hspace{10mm} U = \frac{\sqrt{n} \cdot (\overline{X} - \mu_0)}{S_X}$$

Comparing this with the equation for a simple hypothesis setup, notice that the only change here is the absence of the absolute value symbol. Because this is a one-tailed test, we now care about the direction of statistic. For a **left-tailed** test, we reject $H_0$ if  the test statistic $U \leq c$, while for a **right-tailed** test, we reject $H_0$ if $U \geq c$.  

```{r fig.cap="Rejection Region for a Left-tailed Test"}
q <- qt(1 - 0.05, 5)
shade.dist(
  a = -q,
  b = q,
  param = list(df = 5),
  dist = "dt",
  type = 'lower',
  from = -5,
  to = 5,
  xlab = "U"
)
abline(v = 1, lty = 2)
abline(v = -3, lty = 3)
text(x = 1.8,
     y = 0.3,
     labels = expression(paste("do not\nreject ", H[0])))
text(x = -2.3,
     y = 0.3,
     labels = expression(paste("reject ", H[0])))
title(expression(paste("Critical region for ", H[0]:mu >= mu[0], " at ", alpha == 0.05, ", ", nu == 5)))
```

```{r fig.cap="Rejection Region for a Right-tailed Test"}
q <- qt(1 - 0.05, 5)
shade.dist(
  a = -q,
  b = q,
  param = list(df = 5),
  dist = "dt",
  type = 'upper',
  from = -5,
  to = 5,
  xlab = "U"
)
abline(v = 1, lty = 2)
abline(v = 2.7, lty = 3)
text(x = 1.8,
     y = 0.3,
     labels = expression(paste("do not\nreject ", H[0])))
text(x = 3.5,
     y = 0.3,
     labels = expression(paste("reject ", H[0])))
title(expression(paste("critical region for ", H[0]:mu <= mu[0], " at ", alpha == 0.05, ", ", nu == 5)))
```

### Calculating p-values  

For the one-tailed test, we calculate the p-values based on the alternative hypothesis. For example, if we have $H_1:\mu < \mu_0$, then we are to find:  

$$
\begin{aligned}
\phi(Z) \quad &\text{(for z-test)} \\
T_{n-1}(U)\quad &\text{(for t-test)}
\end{aligned}
$$

On the other hand, for $H_1: \mu > \mu_0$:  

$$
\begin{aligned}
1-\phi(Z) \quad &\text{(for z-test)} \\
1-T_{n-1}(U)\quad &\text{(for t-test)}
\end{aligned}
$$

Note that $\Phi$ and $T$ are the **cumulative distribution functions** or CDFs, and they always give the areas of the left tail. Visualizing the rejection regions will explain why we subract the area from 1 in a right-tailed test.  

<br>

## A Two-Sample t-test  

There are situations when we want to compare two groups and test if their population means are different, or if one is greater than (or less than) relative to the other. In other words, instead of comparing $\mu$ to a reference value $\mu_0$, we are now comparing two population means - $\mu_X$ and $\mu_Y$.  

For a two-sampled test, We now have a new way of calculating the t-statistic. incorporating the new variables $\overline{Y}$ and $\hat{\sigma_Y}$. Below is the formal definition of a two-sample t-statistic:  

$$
\begin{aligned}
U = \frac{(m+n-2)^{1/2}(\overline{X} - \overline{Y})}{(\frac{1}{m} + \frac{1}{n})^{1/2}(S^2_X+S^2_Y)^{1/2}}
\end{aligned}
$$

$\overline{X}$ and $\overline{Y}$ are the respective sample means and $S_X^2$ and $S_Y^2$ are the residual sum of squares or RSS, defined as $S_X^2 = \sum_i^m(X_i - \overline{X})^2$ and $S^2_Y = \sum_j^n(Y_j - \overline{Y})^2$. We use the t-distribution with $m + n - 2$ degrees of freedom for inference in this case.  

## Confidence Intervals  

### What are confidence intervals?    

Confidence intervals give us a range of plausible values for the population parameter based on results from a sample. The conditions for the CLT must also be met for the confidence interval to be valid.  

### Constructing a Confidence Interval  

The confidence interval **for the mean** is computed as:  

$$(\overline{x} - z^* \frac{s}{\sqrt{n}}, ~ \overline{x} + z^* \frac{s}{\sqrt{n}})$$

* $\overline{x}$ is the sample mean  
* $s$ is the sample mean  
* $n$ is the sample size  
* $z^*$ is the critical z-score  

<br>
